# =============================================================================
# de_template: 4-Axis Configuration
# =============================================================================
# Copy this file to .env and fill in your values.
# Run: make print-config   → verify resolved values before starting
# Run: make validate-config → check for incompatible combinations
# =============================================================================

# --- Infra Axes ---
BROKER=redpanda          # redpanda | kafka
CATALOG=hadoop           # hadoop   | rest  (rest needs EXTRA_PROFILES=--profile catalog-rest)
STORAGE=minio            # minio    | aws_s3 | gcs | azure
MODE=batch               # Flink pipeline mode: batch | streaming_bronze

# --- Topic ---
TOPIC=taxi.raw_trips
DLQ_TOPIC=taxi.raw_trips.dlq
DLQ_MAX=0                # max acceptable DLQ messages (0 = none tolerated)
TOPIC_RETENTION_MS=259200000   # 3 days — adjust for your data volume / replay window
DLQ_RETENTION_MS=604800000     # 7 days — longer window to inspect and reprocess failures

# --- Data Source ---
# DATA_PATH: container-internal path (always /data/<filename>).
# HOST_DATA_DIR: host directory mounted to /data/ in the generator container.
#   Default (computed by Makefile): de_template/../data = repo-root data/
#   Override only if your parquet file lives somewhere else:
# HOST_DATA_DIR=/absolute/path/to/your/data
DATA_PATH=/data/yellow_tripdata_2024-01.parquet
MAX_EVENTS=0             # 0 = all events; 10000 for smoke test
GENERATOR_MODE=burst     # Generator send mode: burst | realtime | batch
RATE_LIMIT=0             # burst mode rate cap (events/sec); 0 = unlimited
BATCH_SIZE=1000          # batch mode chunk size
BATCH_DELAY=1.0          # batch mode delay between chunks (seconds)
ALLOW_EMPTY=false        # true only when empty Bronze/Silver outputs are expected
DATASET_NAME=taxi        # expected dataset tag for run_metrics validation
RUN_METRICS_MAX_AGE_MINUTES=120
ALLOW_STALE_RUN_METRICS=false
REQUIRE_RUN_METRICS=true
VALIDATE_SCHEMA=false
SCHEMA_PATH=/schemas/taxi_trip.json
# KEY_FIELD: column name to use as Kafka message key for partition affinity.
# Set to a column that identifies a logical entity (e.g. customer_id, device_id).
# Leave empty for null keys (round-robin partitioning, maximum throughput).
KEY_FIELD=PULocationID

# --- Iceberg table contract ---
BRONZE_TABLE=bronze.raw_trips
SILVER_TABLE=silver.cleaned_trips

# --- Validation thresholds/timeouts ---
BRONZE_COMPLETENESS_RATIO=0.95
WAIT_FOR_SILVER_MIN_ROWS=1
WAIT_FOR_SILVER_TIMEOUT_SECONDS=90
WAIT_FOR_SILVER_POLL_SECONDS=5
HEALTH_HTTP_TIMEOUT_SECONDS=5
HEALTH_DOCKER_TIMEOUT_SECONDS=15
ICEBERG_QUERY_TIMEOUT_SECONDS=90
ICEBERG_METADATA_TIMEOUT_SECONDS=30
DLQ_READ_TIMEOUT_SECONDS=10
DBT_TEST_TIMEOUT_SECONDS=300

# --- Warehouse ---
# Flink + Iceberg use s3a://  |  DuckDB iceberg_scan uses s3://  (both point to same storage)
WAREHOUSE=s3a://warehouse/
S3_ENDPOINT=http://minio:9000
S3_USE_SSL=false
S3_PATH_STYLE=true

# --- DuckDB httpfs (NO http:// prefix — DuckDB httpfs format requirement) ---
DUCKDB_S3_ENDPOINT=minio:9000
DUCKDB_S3_USE_SSL=false
DUCKDB_UNSAFE_ENABLE_VERSION_GUESSING=true

# --- MinIO credentials (local only; for cloud use IAM/Workload Identity — see .env.aws.example) ---
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# --- AWS / S3 credentials (passed through to Flink S3A and DuckDB httpfs) ---
# For local MinIO: use the MinIO root credentials
# For cloud: leave blank and rely on IAM instance profile / Workload Identity
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
AWS_REGION=us-east-1

# --- Project ---
PROJECT=de_pipeline
