-- =============================================================================
-- 07_bronze_streaming.sql.tmpl — Streaming Bronze (continuous Kafka → Iceberg)
-- Generated by: make scaffold DATASET={{ name }}
-- =============================================================================
-- Self-contained: sets streaming session mode, defines Kafka source, creates
-- bronze table (idempotent), and runs the streaming INSERT.
-- Runs indefinitely — cancel from Flink Dashboard or via: make flink-cancel JOB=<id>
-- =============================================================================

-- Streaming session settings (jobs run indefinitely, no dml-sync)
SET 'execution.runtime-mode' = 'streaming';

-- Kafka source: unbounded stream (no scan.bounded.mode — continuous read)
CREATE TABLE IF NOT EXISTS kafka_{{ name }}_raw (
{% for col in columns %}
    {{ col.name | ljust(32) }}{{ col.kafka_type }}{% if col.is_event_ts is not defined or not col.is_event_ts %}{% if col.comment is defined %},          -- {{ col.comment }}{% else %},{% endif %}{% else %},{% endif %}

{% endfor %}
    event_time AS TO_TIMESTAMP({{ event_ts_col }}, '{{ ts_format }}'),
    WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND
) WITH (
    'connector'                     = 'kafka',
    'topic'                         = '{{ topic }}',
    'properties.bootstrap.servers'  = 'broker:9092',
    'properties.group.id'           = 'flink-consumer',
    'scan.startup.mode'             = 'earliest-offset',
    'format'                        = 'json',
    'json.ignore-parse-errors'      = 'true'
);

-- Bronze table DDL (idempotent — same DDL as 05_bronze_batch.sql.tmpl)
CREATE TABLE IF NOT EXISTS iceberg_catalog.{{ bronze_table }} (
{% for col in columns %}
    {{ col.name | ljust(32) }}{{ col.bronze_type }},{% if col.comment is defined %}          -- {{ col.comment }}{% endif %}

{% endfor %}
    ingestion_ts                    TIMESTAMP(3)
) WITH (
    'format-version'                    = '2',
    'write.format.default'              = 'parquet',
    'write.parquet.compression-codec'   = 'snappy'
);

-- Insert stream: parse raw ISO timestamps, append continuously to bronze
INSERT INTO iceberg_catalog.{{ bronze_table }}
SELECT
{% for col in columns %}
{% if col.is_event_ts is defined and col.is_event_ts %}
    TO_TIMESTAMP({{ col.name }}, '{{ ts_format }}') AS {{ col.name }},

{% elif col.bronze_type == 'TIMESTAMP(3)' and col.kafka_type == 'STRING' %}
    TO_TIMESTAMP({{ col.name }}, '{{ ts_format }}') AS {{ col.name }},

{% else %}
    {{ col.name }},

{% endif %}
{% endfor %}
    CURRENT_TIMESTAMP AS ingestion_ts
FROM kafka_{{ name }}_raw;
