-- =============================================================================
-- 05_bronze_batch.sql.tmpl — Bronze Layer: Kafka → Iceberg (batch, raw landing)
-- Generated by: make scaffold DATASET={{ name }}
-- =============================================================================
-- Self-contained: sets session mode, defines Kafka source, creates bronze table,
-- and runs the INSERT. No dependency on 01_source.sql as a separate -i init script.
--
-- *** CRITICAL: Column names must match JSON keys from Kafka EXACTLY (case-sensitive). ***
-- The generator serialises Parquet column names as-is into JSON keys.
-- All snake_case renaming happens in Silver (06_silver.sql.tmpl).
--
-- *** CRITICAL: Timestamp format from Python datetime.isoformat() is 'T'-separated. ***
-- Use '{{ ts_format }}' (note double-quoted T in Flink SQL string literals).
-- =============================================================================

-- Batch session settings (table.dml-sync blocks until INSERT finishes)
SET 'execution.runtime-mode' = 'batch';
SET 'table.dml-sync' = 'true';

-- Kafka source: bounded scan (reads all existing messages and stops)
-- Column names match Parquet/JSON keys EXACTLY (case-sensitive Kafka JSON connector)
CREATE TABLE IF NOT EXISTS kafka_{{ name }}_raw (
{% for col in columns %}
    {{ col.name | ljust(32) }}{{ col.kafka_type }}{% if col.is_event_ts is not defined or not col.is_event_ts %}{% if col.comment is defined %},          -- {{ col.comment }}{% else %},{% endif %}{% else %},{% endif %}

{% endfor %}
    -- Computed: event time for watermarking (ignored in batch mode but required by DDL)
    event_time AS TO_TIMESTAMP({{ event_ts_col }}, '{{ ts_format }}'),
    WATERMARK FOR event_time AS event_time - INTERVAL '{{ watermark_interval_seconds }}' SECOND
) WITH (
    'connector'                     = 'kafka',
    'topic'                         = '{{ topic }}',
    'properties.bootstrap.servers'  = 'broker:9092',
    'properties.group.id'           = 'flink-consumer',
    'scan.startup.mode'             = 'earliest-offset',
    'scan.bounded.mode'             = 'latest-offset',
    'format'                        = 'json',
    'json.ignore-parse-errors'      = 'true'
);

-- Bronze table (unpartitioned — raw append-only landing zone)
CREATE TABLE IF NOT EXISTS iceberg_catalog.{{ bronze_table }} (
{% for col in columns %}
    {{ col.name | ljust(32) }}{{ col.bronze_type }},{% if col.comment is defined %}          -- {{ col.comment }}{% endif %}

{% endfor %}
    ingestion_ts                    TIMESTAMP(3)
) WITH (
    'format-version'                    = '2',
    'write.format.default'              = 'parquet',
    'write.parquet.compression-codec'   = 'snappy'
);

-- Insert: parse raw ISO timestamps, add ingestion timestamp
INSERT INTO iceberg_catalog.{{ bronze_table }}
SELECT
{% for col in columns %}
{% if col.is_event_ts is defined and col.is_event_ts %}
    TO_TIMESTAMP({{ col.name }}, '{{ ts_format }}') AS {{ col.name }},

{% elif col.bronze_type == 'TIMESTAMP(3)' and col.kafka_type == 'STRING' %}
    TO_TIMESTAMP({{ col.name }}, '{{ ts_format }}') AS {{ col.name }},

{% else %}
    {{ col.name }},

{% endif %}
{% endfor %}
    CURRENT_TIMESTAMP AS ingestion_ts
FROM kafka_{{ name }}_raw;
