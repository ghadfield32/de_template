# =============================================================================
# de_template: AWS S3 profile
# =============================================================================
# For AWS S3: no MinIO overlay is started (STORAGE=aws_s3).
# Authentication: prefer IAM roles (ECS task role, EC2 instance profile,
# EKS service account with IRSA) over long-lived access keys.
# See docs/06_secrets_and_auth.md for IAM-first patterns.
# Activate: make env-select ENV=env/aws.env
# =============================================================================

# --- Infra Axes ---
BROKER=kafka             # kafka or redpanda — both work with S3
CATALOG=hadoop           # hadoop | rest
STORAGE=aws_s3           # no storage.minio.yml overlay is loaded
MODE=batch               # Flink pipeline mode: batch | streaming_bronze

# --- Topic ---
TOPIC=taxi.raw_trips
DLQ_TOPIC=taxi.raw_trips.dlq
DLQ_MAX=0
TOPIC_RETENTION_MS=259200000   # 3 days
DLQ_RETENTION_MS=604800000     # 7 days

# --- Data Source ---
DATA_PATH=/data/yellow_tripdata_2024-01.parquet
MAX_EVENTS=0
GENERATOR_MODE=burst     # Generator send mode: burst | realtime | batch
RATE_LIMIT=0
BATCH_SIZE=1000
BATCH_DELAY=1.0
ALLOW_EMPTY=false        # true only when empty Bronze/Silver outputs are expected
DATASET_NAME=taxi         # expected dataset tag for run_metrics validation
RUN_METRICS_MAX_AGE_MINUTES=120
ALLOW_STALE_RUN_METRICS=false
REQUIRE_RUN_METRICS=true
VALIDATE_SCHEMA=false
SCHEMA_PATH=/schemas/taxi_trip.json
KEY_FIELD=PULocationID

# --- Iceberg table contract ---
BRONZE_TABLE=bronze.raw_trips
SILVER_TABLE=silver.cleaned_trips

# --- Validation thresholds/timeouts ---
BRONZE_COMPLETENESS_RATIO=0.95
WAIT_FOR_SILVER_MIN_ROWS=1
WAIT_FOR_SILVER_TIMEOUT_SECONDS=90
WAIT_FOR_SILVER_POLL_SECONDS=5
HEALTH_HTTP_TIMEOUT_SECONDS=5
HEALTH_DOCKER_TIMEOUT_SECONDS=15
ICEBERG_QUERY_TIMEOUT_SECONDS=90
ICEBERG_METADATA_TIMEOUT_SECONDS=30
DLQ_READ_TIMEOUT_SECONDS=10
DBT_TEST_TIMEOUT_SECONDS=300

# --- AWS S3 — replace with your actual bucket ---
WAREHOUSE=s3a://YOUR-BUCKET/warehouse/
S3_ENDPOINT=https://s3.amazonaws.com
S3_USE_SSL=true
S3_PATH_STYLE=false

# --- DuckDB httpfs (NO http:// prefix) ---
DUCKDB_S3_ENDPOINT=s3.amazonaws.com
DUCKDB_S3_USE_SSL=true
DUCKDB_UNSAFE_ENABLE_VERSION_GUESSING=true

# --- Credentials ---
# For EC2/ECS/EKS with IAM role: leave blank (auto-discovered via DefaultAWSCredentialsProviderChain)
# For CI/CD or local dev: inject via environment or AWS Secrets Manager
# Never commit real credentials to git
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1

# --- Project ---
PROJECT=de_pipeline
