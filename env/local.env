# =============================================================================
# de_template: Local development profile (MinIO + Redpanda)
# =============================================================================
# Default profile: fully self-contained, no cloud credentials required.
# Activate: make env-select ENV=env/local.env
# =============================================================================

# --- Infra Axes ---
BROKER=redpanda          # redpanda | kafka
CATALOG=hadoop           # hadoop   | rest
STORAGE=minio            # minio    | aws_s3 | gcs | azure
MODE=batch               # batch    | streaming_bronze

# --- Topic ---
TOPIC=taxi.raw_trips
DLQ_TOPIC=taxi.raw_trips.dlq
DLQ_MAX=0                # max acceptable DLQ messages (0 = none tolerated)

# --- Data Source ---
# DATA_PATH: container-internal path (always /data/<filename>).
# HOST_DATA_DIR: host directory mounted to /data/ in the generator container.
#   Default (computed by Makefile): de_template/../data
#   Override only if your parquet file lives somewhere else:
# HOST_DATA_DIR=/absolute/path/to/your/data
DATA_PATH=/data/yellow_tripdata_2024-01.parquet
MAX_EVENTS=0             # 0 = all events; 10000 for smoke test

# --- Warehouse ---
# Flink + Iceberg use s3a://  |  DuckDB iceberg_scan uses s3://  (both point to same storage)
WAREHOUSE=s3a://warehouse/
S3_ENDPOINT=http://minio:9000
S3_USE_SSL=false
S3_PATH_STYLE=true

# --- DuckDB httpfs (NO http:// prefix â€” DuckDB httpfs format requirement) ---
DUCKDB_S3_ENDPOINT=minio:9000
DUCKDB_S3_USE_SSL=false

# --- MinIO credentials (local only; for cloud use IAM/Workload Identity) ---
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# --- AWS / S3 credentials (passed through to Flink S3A and DuckDB httpfs) ---
# For local MinIO: use the MinIO root credentials above
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
AWS_REGION=us-east-1

# --- Project ---
PROJECT=de_pipeline
